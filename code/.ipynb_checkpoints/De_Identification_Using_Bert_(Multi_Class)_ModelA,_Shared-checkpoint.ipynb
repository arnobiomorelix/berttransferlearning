{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4Hw4aOBsECR"
   },
   "source": [
    "# Setup\n",
    "based on https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=191zq3ZErihP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "Q-h6Nk0WsICS",
    "outputId": "b429ee57-d02c-4003-dc41-a26cfc44299d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU address is grpc://10.108.13.194:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 06:19:30.392383 140151232075648 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU devices:\n",
      "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 861201113876687645),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7509863777384350548),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10957324747785543829),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16010609198301123736),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15642925627654617544),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 202491004858483873),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7185590080412384499),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9450033752344641433),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5937089048049227403),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16704279036719683882),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1360056868896268412)]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "print('TPU address is', TPU_ADDRESS)\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "with tf.Session(TPU_ADDRESS) as session:\n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(session.list_devices())\n",
    "\n",
    "  # Upload credentials to TPU.\n",
    "  with open('/content/adc.json', 'r') as f:\n",
    "    auth_info = json.load(f)\n",
    "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "  # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8Dxp0OwI_9X"
   },
   "source": [
    "# Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eqCtk6_OBt3"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "SFwSCJIsJFkB",
    "outputId": "0a121f29-6208-4e94-833f-41b40586e501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#This file is written to ingest data from i2b2\n",
    "#below are the requried library\n",
    "from xml.dom import minidom # need this to read xlm files\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import DataFrame\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt') #this package needs to be downloaded separately\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tSyXkHRwdPZ"
   },
   "source": [
    "### GDrive Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsap3HvwNtrA"
   },
   "source": [
    "We first mount the google drive containing the training and test fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "0dp7AABnNcce",
    "outputId": "e687e8e4-e8b4-4763-8aac-181f1230b1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive # this sets the file path to your personal google drive. You will need to enter the authorization code each time. \n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCX9KxgT2zy7"
   },
   "source": [
    "## Creating List of Files to be Ingested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hqr9iRr1N1h0"
   },
   "source": [
    "We then created two list - each contains the list of the file names for training and testing.\n",
    "\n",
    "1.   train_filelist = 790 EHR records\n",
    "2.   test_filelist=514 EHR records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "lMonhsbPNgDi",
    "outputId": "e5af7285-fb09-496a-f394-58ab9e190bb4"
   },
   "outputs": [],
   "source": [
    "# data processing created with the help of teaching assistant Sudha Subramanian, who previously worked with the same dataset\n",
    "\n",
    "train_filelist=[]\n",
    "\n",
    "for file in os.listdir('/gdrive/My Drive/w266_NLP/training-PHI'):#set your file path here\n",
    "  filename = os.fsdecode(os.fsencode('/gdrive/My Drive/w266_NLP/training-PHI/'+file))\n",
    "  if filename.endswith( ('.xml') ): # select xml files\n",
    "    train_filelist.append(filename)\n",
    "\n",
    "print(\"There are {} training file\".format(len(train_filelist))) #check that the number of training file is 790 records for 178 patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "MpT-K7y1NlhM",
    "outputId": "e48b58e9-1791-4fcf-8f19-66e73f56a472"
   },
   "outputs": [],
   "source": [
    "test_filelist=[]\n",
    "\n",
    "for file in os.listdir('/gdrive/My Drive/w266_NLP/test-PHI'):#set your file path here\n",
    "  filename = os.fsdecode(os.fsencode('/gdrive/My Drive/w266_NLP/test-PHI/'+file))\n",
    "  if filename.endswith( ('.xml') ): # select xml files\n",
    "    test_filelist.append(filename)\n",
    "\n",
    "print(\"There are {} test file\".format(len(test_filelist))) #check that the number of test file is 514 records for 178 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHkiAyKCOOfY"
   },
   "source": [
    "# Process Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46iA8aDpOecR"
   },
   "source": [
    "The tag generator process the annotation into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-mR_51sNr7O"
   },
   "outputs": [],
   "source": [
    "def tag_generator(file):\n",
    "  '''The function extract the tags from the EHR record and turn them into pd dataframe'''\n",
    "  tree = ET.parse(file)\n",
    "  root=tree.getroot()\n",
    "  \n",
    "  PHI_category=['NAME','PROFESSION','LOCATION','AGE','DATE','CONTACT','ID']# Here are the seven PHI category defined by i2b2\n",
    "  #PHI_category=[category]\n",
    "  tag_list=[]#An empty list to hold all dictionary items\n",
    "  for category in PHI_category:\n",
    "    for tag in root.iter():\n",
    "      if tag.tag==category:#skip if a specific tag is not found\n",
    "          tag.attrib['Category']=category #add a column on category\n",
    "          tag.attrib['File']=file[len(file)-10:len(file)-4] # add a column to indicate file name\n",
    "          tag_list.append(tag.attrib)\n",
    "  temp_df=pd.DataFrame(tag_list)\n",
    "      \n",
    "  return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZMzXvl6iPix"
   },
   "outputs": [],
   "source": [
    "def note_generator(file):\n",
    "  #'''This function breakdown inidividaul EHR text note into sentences using XML tags, divided by new line and period'''\n",
    "    tree = ET.ElementTree(file=file)\n",
    "    root = tree.getroot()\n",
    "    all_notes = []\n",
    "\n",
    "    text = root.find('TEXT').text\n",
    "    sentences = [sent.split('\\n') for sent in sent_tokenize(text) if sent!='\\n']\n",
    "    \n",
    "\n",
    "    for text in sentences:#this part ignore empty lines\n",
    "        for sub_item in text:\n",
    "            if sub_item.replace(' ','') != '':\n",
    "                all_notes.append(sub_item)    \n",
    "    \n",
    "    return all_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REVnrHt7gMsN"
   },
   "source": [
    "### Install Bert tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "tEc2dLtQ5HPt",
    "outputId": "91abd275-c7f4-4ff5-dcf6-1f429f92adc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 06:24:04.819093 140151232075648 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow    # this replaces the bert github clone\n",
    "!pip install keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "vLTuU4VQ4geo",
    "outputId": "0aa614e7-6afe-4300-e76c-7c8f082f25f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 06:24:20.159090 140151232075648 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use the case model here\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xN-hCdU7qF9"
   },
   "outputs": [],
   "source": [
    "def sentence_encoding(file):#this function is looped within the token_annotator function\n",
    "  \n",
    "  sentence_list=note_generator(file) #generate a list of sentences from tex\n",
    "  \n",
    "  df=tag_generator(file)\n",
    "  text_list=df['text'].tolist() #generate a list of tag \"TEXT\"\n",
    "  type_list=df['TYPE'].tolist() #generate a list of tag \"type\"\n",
    "  category_list=df['Category'].tolist() #generate a list of tag \"category\"\n",
    "  \n",
    "  processed_sentence=[]\n",
    "  processed_text=[]\n",
    "  processed_type=[]\n",
    "  processed_category=[]\n",
    "  \n",
    "  def findWholeWord(w):#this function finds a word within a string broken down by regular expression (case sensitive)\n",
    "    return re.compile(r'\\b({0})\\b'.format(w)).search\n",
    "  \n",
    "  for sentence in sentence_list:\n",
    "     for text in text_list:\n",
    "        if findWholeWord(text)(sentence)!=None:\n",
    "          processed_sentence.append(sentence)\n",
    "          processed_text.append(text)\n",
    "          processed_type.append(type_list[text_list.index(text)])\n",
    "          processed_category.append(category_list[text_list.index(text)])\n",
    "\n",
    "  \n",
    "  temp_df=pd.DataFrame({'Sentence':processed_sentence, 'Word':processed_text, 'Type':processed_type, 'Category':processed_category})\n",
    "  df = temp_df.drop_duplicates()\n",
    "        \n",
    "  return df\n",
    "  #return sentence_list, text_list, type_list, category_list\n",
    "  #return processed_sentence, processed_text, processed_type\n",
    "\n",
    "# sentence_encoding(train_filelist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQon1Y3AEfwI"
   },
   "outputs": [],
   "source": [
    "def token_annotator(file):\n",
    "  \n",
    "  temp_df=sentence_encoding(file)#take the data frame and turn them into individual lists\n",
    "  \n",
    "  type_list=temp_df['Type'].tolist()\n",
    "  temp_sentence_list=temp_df['Sentence'].tolist()\n",
    "  word_list=temp_df['Word'].tolist()\n",
    "  temp_unique_sentence_list=set(temp_sentence_list)\n",
    "  sentence_list=list(temp_unique_sentence_list) #take out duplicate sentences\n",
    "  \n",
    "  tokenized_word=[] #separate individual text into words (e.g, Mia E. Tapia to \"Mia\",\"E.\",\"Tapia\")\n",
    "  for phrase in word_list:\n",
    "    tokenized_word.append(tokenizer.tokenize(phrase))\n",
    "  \n",
    "  tokenized_sentence=[]\n",
    "  encoded_token=[]\n",
    "  \n",
    "  for i in range(len(sentence_list)): #tokenize the sentence and encode individual word\n",
    "    token_list=tokenizer.tokenize(sentence_list[i])\n",
    "    tokenized_sentence.append(token_list)\n",
    "    temp_list=['O' for length in range(len(token_list))]\n",
    "    for j in range(len(tokenized_word)):\n",
    "      if all(elem in token_list for elem in tokenized_word[j])==True:\n",
    "        #print(token_list, tokenized_word[j])\n",
    "        for word in tokenized_word[j]:\n",
    "          temp_list[token_list.index(word)]=(type_list[j])\n",
    "          #print(temp_list)\n",
    "    encoded_token.append(temp_list)\n",
    "          \n",
    "  return tokenized_sentence,encoded_token\n",
    "\n",
    "\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q4gnf_ESsIC_",
    "outputId": "476f5c3e-1253-4951-97f3-41f8da5f095f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 12 12\n"
     ]
    }
   ],
   "source": [
    "# a,b=token_annotator(train_filelist[3])\n",
    "# print(len(train_filelist[3]), len(a), len(b))\n",
    "# # BERT_Token_list=[]\n",
    "# # for i in range(2):#789 for train 513 for test\n",
    "# #   tokenized_sentence_list, encoded_token=token_annotator(train_filelist[i])\n",
    "# #   for j in range(len(train_filelist[i])):\n",
    "# #     BERT_Token_list.extend(encoded_token[j])\n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_UJDLnnoPQe"
   },
   "outputs": [],
   "source": [
    "def type_token_generator(file): \n",
    "  #this function convert all the text of a record into individual BERT tokenized list and generate type encoding list\n",
    "  all_sentences=note_generator(file)\n",
    "  tokenized_sentences=[]\n",
    "  for sentence in all_sentences:\n",
    "    tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "  \n",
    "  type_token=[]\n",
    "    \n",
    "  sentence_list, encoded_token=token_annotator(file)\n",
    "  \n",
    "  for sentence in tokenized_sentences:\n",
    "    if sentence in sentence_list:\n",
    "      type_token.append(encoded_token[sentence_list.index(sentence)])\n",
    "    else:\n",
    "      type_token.append(['O'for i in range(len(sentence))])\n",
    "  \n",
    "  label_list=[]\n",
    "  label_dict={\"O\":0, \"DATE\":1, \"DOCTOR\":2,\"HOSPITAL\":3,'PATIENT':4,'AGE':5,'MEDICALRECORD':6,'CITY':7,'STATE':8,'PHONE':9,'USERNAME':10,'IDNUM':11,'PROFESSION':12,'STREET':13,'ZIP':14,'ORGANIZATION':15,'COUNTRY':16,'FAX':17,'DEVICE':18,'EMAIL':19,'LOCATION-OTHER':20,'URL':21,'HEALTHPLAN':22,'BIOID':23}# ,'IPADDRESS':24,'ACCOUNT NUMBER':25}\n",
    "  for type_list in type_token:# we convert the label to numerical for Bert training. We can add types here later. \n",
    "    label_list.append([label_dict.get(item,item)  for item in type_list])\n",
    "    #label_list.append([0 if typetoken =='O' else 1 for typetoken in type_list])\n",
    "\n",
    "\n",
    "  #return tokenized_sentences, type_token, label_list\n",
    "  return tokenized_sentences, type_token, label_list #take a look at segment of the list to make sure the they are corect\n",
    "# we were missing tokenized_sentences, type_token from the return, not sure why\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp90C_1bwkVI"
   },
   "source": [
    "We use the cell below to double check that the encoding is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "u5Tizd7Hv4Ny",
    "outputId": "c3b5cda9-2fd2-4d22-ba8a-553b39a4f4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['February', '12', ',', '210', '##6'] ['DATE', 'DATE', 'DATE', 'DATE', 'DATE'] [1, 1, 1, 1, 1]\n",
      "65 5 65 49\n"
     ]
    }
   ],
   "source": [
    "#Check that the label is correct\n",
    "x, y,z=type_token_generator(train_filelist[0])\n",
    "num=7\n",
    "\n",
    "print(x[num],y[num],z[num])\n",
    "print(len(x), len(y[num]), len(z), len(train_filelist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTF7RWWNzWkG"
   },
   "outputs": [],
   "source": [
    "#calculate the number of sentence length\n",
    "BERT_Sentence_Length=[]\n",
    "\n",
    "for i in range(789):#789 for train 513 for test\n",
    "  x, y, z= type_token_generator(train_filelist[i])\n",
    "  for j in range(len(y)):\n",
    "    BERT_Sentence_Length.append(len(y[j]))\n",
    "\n",
    "#print(BERT_Sentence_Length)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3tkcmtuc0fR_",
    "outputId": "15c03fc6-037d-48d4-ddfd-9e8dd5fc97dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 501,\n",
       "         1: 955,\n",
       "         2: 4709,\n",
       "         3: 3788,\n",
       "         4: 4186,\n",
       "         5: 4169,\n",
       "         6: 4306,\n",
       "         7: 4410,\n",
       "         8: 4054,\n",
       "         9: 4525,\n",
       "         10: 3465,\n",
       "         11: 3152,\n",
       "         12: 2919,\n",
       "         13: 2639,\n",
       "         14: 2497,\n",
       "         15: 2276,\n",
       "         16: 2111,\n",
       "         17: 1905,\n",
       "         18: 1685,\n",
       "         19: 1457,\n",
       "         20: 1345,\n",
       "         21: 1174,\n",
       "         22: 1008,\n",
       "         23: 975,\n",
       "         24: 840,\n",
       "         25: 934,\n",
       "         26: 664,\n",
       "         27: 633,\n",
       "         28: 593,\n",
       "         29: 475,\n",
       "         30: 560,\n",
       "         31: 395,\n",
       "         32: 332,\n",
       "         33: 355,\n",
       "         34: 290,\n",
       "         35: 278,\n",
       "         36: 259,\n",
       "         37: 216,\n",
       "         38: 199,\n",
       "         39: 208,\n",
       "         40: 173,\n",
       "         41: 159,\n",
       "         42: 142,\n",
       "         43: 157,\n",
       "         44: 94,\n",
       "         45: 171,\n",
       "         46: 91,\n",
       "         47: 83,\n",
       "         48: 77,\n",
       "         49: 56,\n",
       "         50: 77,\n",
       "         51: 54,\n",
       "         52: 78,\n",
       "         53: 47,\n",
       "         54: 46,\n",
       "         55: 66,\n",
       "         56: 22,\n",
       "         57: 53,\n",
       "         58: 46,\n",
       "         59: 35,\n",
       "         60: 25,\n",
       "         61: 26,\n",
       "         62: 26,\n",
       "         63: 36,\n",
       "         64: 25,\n",
       "         65: 24,\n",
       "         66: 25,\n",
       "         67: 12,\n",
       "         68: 12,\n",
       "         69: 20,\n",
       "         70: 14,\n",
       "         71: 20,\n",
       "         72: 16,\n",
       "         73: 8,\n",
       "         74: 12,\n",
       "         75: 21,\n",
       "         76: 12,\n",
       "         77: 18,\n",
       "         78: 9,\n",
       "         79: 7,\n",
       "         80: 5,\n",
       "         81: 9,\n",
       "         82: 4,\n",
       "         83: 9,\n",
       "         84: 6,\n",
       "         85: 8,\n",
       "         86: 9,\n",
       "         87: 1,\n",
       "         88: 4,\n",
       "         89: 6,\n",
       "         90: 2,\n",
       "         91: 2,\n",
       "         92: 7,\n",
       "         93: 2,\n",
       "         94: 4,\n",
       "         95: 3,\n",
       "         96: 5,\n",
       "         97: 3,\n",
       "         98: 2,\n",
       "         99: 3,\n",
       "         100: 2,\n",
       "         101: 5,\n",
       "         102: 2,\n",
       "         103: 4,\n",
       "         104: 1,\n",
       "         105: 4,\n",
       "         106: 3,\n",
       "         107: 3,\n",
       "         108: 2,\n",
       "         110: 1,\n",
       "         112: 2,\n",
       "         113: 5,\n",
       "         114: 1,\n",
       "         115: 2,\n",
       "         116: 3,\n",
       "         117: 4,\n",
       "         118: 2,\n",
       "         120: 1,\n",
       "         121: 1,\n",
       "         122: 4,\n",
       "         123: 2,\n",
       "         124: 1,\n",
       "         126: 2,\n",
       "         128: 2,\n",
       "         130: 2,\n",
       "         131: 1,\n",
       "         132: 1,\n",
       "         133: 1,\n",
       "         135: 1,\n",
       "         136: 1,\n",
       "         137: 2,\n",
       "         141: 1,\n",
       "         142: 1,\n",
       "         146: 2,\n",
       "         148: 1,\n",
       "         150: 1,\n",
       "         151: 1,\n",
       "         155: 1,\n",
       "         157: 2,\n",
       "         158: 1,\n",
       "         165: 1,\n",
       "         167: 1,\n",
       "         170: 1,\n",
       "         174: 1,\n",
       "         184: 2,\n",
       "         186: 1,\n",
       "         192: 1,\n",
       "         201: 1,\n",
       "         211: 1,\n",
       "         212: 1,\n",
       "         213: 3,\n",
       "         251: 1,\n",
       "         256: 1,\n",
       "         265: 1,\n",
       "         267: 1,\n",
       "         298: 1,\n",
       "         338: 1,\n",
       "         349: 1,\n",
       "         355: 1,\n",
       "         385: 1,\n",
       "         394: 1,\n",
       "         437: 1,\n",
       "         441: 1,\n",
       "         459: 1,\n",
       "         472: 1,\n",
       "         536: 1,\n",
       "         546: 1,\n",
       "         754: 1,\n",
       "         846: 1,\n",
       "         917: 1,\n",
       "         1175: 1})"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "length_count=Counter()\n",
    "for length in BERT_Sentence_Length:\n",
    "    length_count[length]+=1\n",
    "\n",
    "length_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYT9OSnVur-Y"
   },
   "outputs": [],
   "source": [
    "#Calculate the number of token type\n",
    "\n",
    "BERT_Token_list=[]\n",
    "for i in range(513):#789 for train 513 for test\n",
    "  x, y, z= type_token_generator(test_filelist[i])\n",
    "  for j in range(len(y)):\n",
    "    BERT_Token_list.extend(y[j])\n",
    "#print(BERT_Token_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "F3vAnikEvvim",
    "outputId": "b04b341a-87e7-4c3a-e8bf-6ca0a5de1f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'AGE': 905,\n",
       "         'CITY': 666,\n",
       "         'COUNTRY': 200,\n",
       "         'DATE': 16888,\n",
       "         'DEVICE': 37,\n",
       "         'DOCTOR': 6667,\n",
       "         'EMAIL': 11,\n",
       "         'HOSPITAL': 3415,\n",
       "         'IDNUM': 1198,\n",
       "         'LOCATION-OTHER': 28,\n",
       "         'MEDICALRECORD': 1952,\n",
       "         'O': 585508,\n",
       "         'ORGANIZATION': 250,\n",
       "         'PATIENT': 3330,\n",
       "         'PHONE': 663,\n",
       "         'PROFESSION': 491,\n",
       "         'STATE': 279,\n",
       "         'STREET': 570,\n",
       "         'USERNAME': 288,\n",
       "         'ZIP': 419})"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "type_count=Counter()\n",
    "for PHI_type in BERT_Token_list:\n",
    "    type_count[PHI_type]+=1\n",
    "\n",
    "type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKyZe-ME-QvS"
   },
   "outputs": [],
   "source": [
    "#DELETE - Check that the dictionary label functions properly\n",
    "# unique_list=[]\n",
    "\n",
    "# for i in range(789):#use 789 for train and 513 for test\n",
    "#   label_list=type_token_generator(train_filelist[i])\n",
    "#   for j in range(len(label_list)):\n",
    "#     for item in label_list[j]:\n",
    "#       if item not in unique_list:\n",
    "#         unique_list.append(item)\n",
    "    \n",
    "# print(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MvR8jWAr4wzK"
   },
   "outputs": [],
   "source": [
    "#unique_list.sort()\n",
    "#print(unique_list)#we have 24 classes for training (0-23)\n",
    "#test=[0, 1, 4, 6, 5, 3, 12, 2, 9, 11, 10, 13, 7, 8, 14, 18, 16, 15, 20, 19]\n",
    "#train=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eztMvTDLey21"
   },
   "source": [
    "### Generating BERT array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXrQFz9jNbCC"
   },
   "outputs": [],
   "source": [
    "def bert_array(file, max_seq_length):\n",
    "  '''This function generates the 5 lists of array that is required to feed into the model'''\n",
    "  \n",
    "  token_sentence, type_token, label_list= type_token_generator(file)\n",
    "  \n",
    "  token_list=[]\n",
    "  input_IDs=[]\n",
    "  input_mask=[]#1 for non padding and 0 for padding\n",
    "  segment_ID=[]\n",
    "  label=[]\n",
    "  \n",
    "  for untrimmed_sentence in token_sentence:\n",
    "    sentence=untrimmed_sentence[0:(max_seq_length)-2] #trim the list to allow space for CLS and SEP\n",
    "    sentence.insert(0,'[CLS]')\n",
    "    sentence.insert(len(sentence),'[SEP]')\n",
    "    length_before_padding=len(sentence)\n",
    "    temp_inputID=[1 for i in range(length_before_padding)]#insert 1 for [CLS] and [SEP] for mask\n",
    "    sentence.extend(['[PAD]' for i in range(max_seq_length-len(sentence))])\n",
    "    temp_inputID.extend([0 for i in range(max_seq_length-len(temp_inputID))])\n",
    "    token_list.append(sentence)\n",
    "    input_mask.append(temp_inputID)\n",
    "    segment_ID.append([0 for i in range(max_seq_length)])\n",
    "  \n",
    "  for token in token_list:\n",
    "    input_ids=tokenizer.convert_tokens_to_ids(token)\n",
    "    input_IDs.append(input_ids)\n",
    "  \n",
    "  for untrimmed_item in label_list:\n",
    "    item=untrimmed_item[0:(max_seq_length-2)]#trim the list to allow space for CLS and SEP\n",
    "    item.insert(0,24)#class label 24 for CLS (Arnobio - you need to change 24 to 0 for binary)\n",
    "    item.insert(len(item),25) #class label 25 for SEP (Arnobio - you need to change 25 to 0 for binary)\n",
    "    item.extend([26 for i in range(max_seq_length-len(item))])  #class label 26 represents paddinging (ARnobio you need to change 26 to 0 for binary)\n",
    "    label.append(item)\n",
    "  \n",
    "  \n",
    "  return token_list, input_IDs, input_mask, segment_ID, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# token='this'\n",
    "# input_ids = tokenizer.convert_tokens_to_ids(['[CLS]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6gqCZXFbFmF"
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 32\n",
    "\n",
    "# create data\n",
    "max_seq_length = 20\n",
    "\n",
    "# num of files to retrieve\n",
    "num_of_file_train = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjhndmGlkYY_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_train_data(num_of_file, max_seq_length):#Max number of file number is 789\n",
    "  '''This function runs through a loop to append the tokens, input ids, input masks, segement id and labels to 5 individual np arrays '''\n",
    "  temp_list0, temp_list1, temp_list2, temp_list3, temp_list4=[],[],[],[],[]\n",
    "  for i in range(num_of_file):\n",
    "    temp_data= bert_array(train_filelist[i],max_seq_length)\n",
    "    \n",
    "    for j in range(len(temp_data[0])):\n",
    "      temp_list0.append(temp_data[0][j])\n",
    "      temp_list1.append(temp_data[1][j])\n",
    "      temp_list2.append(temp_data[2][j])\n",
    "      temp_list3.append(temp_data[3][j])\n",
    "      temp_list4.append(temp_data[4][j])\n",
    "  \n",
    "#   np_token_list=np.array(temp_list0)\n",
    "#   np_input_ids=np.array(temp_list1)\n",
    "#   np_input_masks=np.array(temp_list2)\n",
    "#   np_segment_ids=np.array(temp_list3)\n",
    "#   np_labels=np.array(temp_list4)\n",
    "      \n",
    "  #return np_token_list, np_input_ids, np_input_masks, np_segment_ids, np_labels\n",
    "  return temp_list0, temp_list1, temp_list2, temp_list3, temp_list4\n",
    "\n",
    "#change number of file here (MAX:789)\n",
    "\n",
    "\n",
    "train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_labels= generate_train_data(num_of_file_train,max_seq_length)\n",
    "\n",
    "#print(train_token_list)\n",
    "#check that the shape is correct\n",
    "#print(train_input_ids.shape, train_input_masks.shape, train_token_list.shape, train_segment_ids.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HKxQZDQSUnMg",
    "outputId": "451e8a4b-9c21-4a4d-c608-8b3b2f7b4252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17470"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcJYPa7guYAv"
   },
   "outputs": [],
   "source": [
    "# x=np.array([train_token_list, train_input_ids, train_input_masks])\n",
    "\n",
    "# print(x)\n",
    "\n",
    "# y=[[123],[123],[456]]\n",
    "# z=[['a,b,c'],['w','b','f'],['r,t,g']]\n",
    "# w=[['sf'],['sdf'],['34,5,2']]\n",
    "\n",
    "# a=np.array(y)\n",
    "# b=np.array(z)\n",
    "# c=np.array(w)\n",
    "# d=[a,b,c]\n",
    "\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_6jcasSriGD"
   },
   "outputs": [],
   "source": [
    "num_of_file_test = 100\n",
    "\n",
    "def generate_test_data(num_of_file, max_seq_length):#Max number of file is 513\n",
    "  '''This function runs through a loop to append the tokens, input ids, input masks, segement id and labels to 5 individual np arrays '''\n",
    "  temp_list0, temp_list1, temp_list2, temp_list3, temp_list4=[],[],[],[],[]\n",
    "  for i in range(num_of_file):\n",
    "    temp_data= bert_array(test_filelist[i],max_seq_length)\n",
    "    \n",
    "    for j in range(len(temp_data[0])):\n",
    "      temp_list0.append(temp_data[0][j])\n",
    "      temp_list1.append(temp_data[1][j])\n",
    "      temp_list2.append(temp_data[2][j])\n",
    "      temp_list3.append(temp_data[3][j])\n",
    "      temp_list4.append(temp_data[4][j])\n",
    "  \n",
    "#   np_token_list=np.array(temp_list0)\n",
    "#   np_input_ids=np.array(temp_list1)\n",
    "#   np_input_masks=np.array(temp_list2)\n",
    "#   np_segment_ids=np.array(temp_list3)\n",
    "#   np_labels=np.array(temp_list4)\n",
    "      \n",
    "  #return np_token_list, np_input_ids, np_input_masks, np_segment_ids, np_labels\n",
    "  return temp_list0, temp_list1, temp_list2, temp_list3, temp_list4\n",
    "\n",
    "test_token_list, test_input_ids, test_input_masks, test_segment_ids, test_labels = generate_test_data(num_of_file_test,max_seq_length)\n",
    "#print(test_input_ids.shape, test_input_masks.shape, test_token_list.shape, test_segment_ids.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hMUhd6L2XXgk",
    "outputId": "e2d400fa-402b-4128-ac09-5eef51628bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8444\n"
     ]
    }
   ],
   "source": [
    "# # print(train_token_list[0])\n",
    "# print(train_input_ids[0])\n",
    "print(len(test_input_ids))\n",
    "# # print(train_segment_ids[0])\n",
    "# #print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dMZ4TZn6kPC9",
    "outputId": "c50c6dba-33b3-4006-d7a7-7ad552b4de05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 0, 0, 0, 1, 1, 1, 1, 0, 1, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz0XLARdkY-u"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJopx8eaXwdA"
   },
   "source": [
    "### Save Data Arrays for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sbqFX-tNSdxh",
    "outputId": "f71e42e0-954c-40e7-b4bc-71a491645bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "# save tokens outside of notebook\n",
    "%cd ..\n",
    "np.save(\"train_token_list\",train_token_list)\n",
    "np.save(\"train_input_ids\",train_token_list)\n",
    "np.save(\"train_input_masks\",train_token_list)\n",
    "np.save(\"train_segment_ids\",train_token_list)\n",
    "np.save(\"train_labels\",train_token_list)\n",
    "\n",
    "\n",
    "np.save(\"test_token_list\",test_token_list)\n",
    "np.save(\"test_input_ids\", test_input_ids)\n",
    "np.save(\"test_input_masks\",test_input_masks)\n",
    "np.save(\"test_segment_ids\",test_segment_ids)\n",
    "np.save(\"test_labels\",test_labels)\n",
    "#%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S68GlWkLdQNQ"
   },
   "outputs": [],
   "source": [
    "# download numpy arrays to local machine\n",
    "from google.colab import files\n",
    "\n",
    "# download train arrays\n",
    "files.download('train_token_list.npy')\n",
    "files.download('train_input_ids.npy')\n",
    "files.download('train_input_masks.npy')\n",
    "files.download('train_segment_ids.npy')\n",
    "files.download('train_labels.npy')\n",
    "\n",
    "# download test arrays\n",
    "files.download('test_token_list.npy')\n",
    "files.download('test_input_ids.npy')\n",
    "files.download('test_input_masks.npy')\n",
    "files.download('test_segment_ids.npy')\n",
    "files.download('test_labels.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# files manually uploaded to drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-q4Pn_DYLFW"
   },
   "source": [
    "### Load Data Arrays from Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "colab_type": "code",
    "id": "HSyNIcbGdO1r",
    "outputId": "29e4cc7b-1c73-4b02-b9a6-f8025fbcaf06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0731 05:37:06.803610 139950122833792 ultratb.py:152] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-92-2f74130f9697>\", line 1, in <module>\n",
      "    get_ipython().magic('cd /gdrive')\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-91>\", line 2, in cd\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/osm.py\", line 288, in cd\n",
      "    oldcwd = py3compat.getcwd()\n",
      "OSError: [Errno 107] Transport endpoint is not connected\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "OSError: [Errno 107] Transport endpoint is not connected\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# saved arrays are here https://drive.google.com/drive/u/1/folders/18uQWrQ5VO2tERtDg8VKcZUsiO0r6RhT2\n",
    "\n",
    "#from google.colab import drive # this sets the file path to your personal google drive. You will need to enter the authorization code each time. \n",
    "#drive.mount('/gdrive')\n",
    "cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVu-qvw1d-aF"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPGcqi9HwriM"
   },
   "source": [
    "#Building the Bert Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_D-tnIRxTNt"
   },
   "source": [
    "## Building out model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h__HPbLpEcPf"
   },
   "outputs": [],
   "source": [
    "# Partially based on and created with teh help with Joachim Rahmfeld and his work, as well as \"BERT in Keras with Tensorflow hub\" (https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b) \n",
    "\n",
    "#BERT_MODEL_HUB\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=3,\n",
    "        pooling=\"sequence_output\",\n",
    "        bert_path=BERT_MODEL_HUB,\n",
    "        #bert_path=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        #             trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "        ]\n",
    "        \n",
    "        trainable_layers = []\n",
    "        #         else:\n",
    "        #             raise NameError(\n",
    "        #                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "        #             )\n",
    "\n",
    "                # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"pooled_output\"\n",
    "        #             ]\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        #             result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"sequence_output\"\n",
    "        #             ]\n",
    "\n",
    "        #             mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        #             masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "        #                     tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "        #             input_mask = tf.cast(input_mask, tf.float32)\n",
    "        #             pooled = masked_reduce_mean(result, input_mask)\n",
    "        #         else:\n",
    "        #             raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "                \n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "        \n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJMdas5-xufv"
   },
   "outputs": [],
   "source": [
    "\n",
    "#num_labels = 25\n",
    "#Here we build a custom loss function for our classes\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "#     y_true: Shape: (batch x (max_length + 1) )\n",
    "#     y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "#     returns:  cost\n",
    "#     \"\"\"\n",
    "\n",
    "#     #get labels and predictions\n",
    "    \n",
    "#     y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "#     print(y_label)\n",
    "  \n",
    "    \n",
    "#     mask = (y_label < 24)  #CLS=24, SEP=25, PAD=26 \n",
    "\n",
    "#     y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "   \n",
    "#     y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 27])\n",
    " \n",
    "    \n",
    "#     y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "#     return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dWTr-9nnmay"
   },
   "outputs": [],
   "source": [
    "num_labels = 27\n",
    "\n",
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 24)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, num_labels]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "61Mq67DfzCQ8",
    "outputId": "e629fd46-732c-4e10-9422-17a1028234f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_42:0\", shape=(2, 1), dtype=int32)\n",
      "Tensor(\"Reshape_42:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"boolean_mask_22/GatherV2:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"Reshape_43:0\", shape=(2, 27), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# y_true = tf.constant([[27],[0]])\n",
    "# print(y_true)\n",
    "# y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "# print(y_label)\n",
    "# mask = (y_label < 24)\n",
    "# y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "# print(y_label_masked)\n",
    "# y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 27])\n",
    "# print(y_flat_pred)\n",
    "# y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "GavDIlNJHN-l",
    "outputId": "51072d82-76e3-4e41-86fe-814f758e57ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 1), dtype=int32)\n",
      "Tensor(\"Const_1:0\", shape=(2, 27), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(2,), dtype=int32)\n",
      "0.84729946\n"
     ]
    }
   ],
   "source": [
    "# #Check that custom_loss works WHY DOESNT THIS WORK!!!????\n",
    "# sess.close()\n",
    "# y_true = tf.constant([[28],[0]]) #0.5108 should the correct answer\n",
    "# print(y_true)\n",
    "\n",
    "# y_pred = tf.constant([\n",
    "#     [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0.4,0,0,0,0,0,.4,0,0.5,0],\n",
    "#     [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0],\n",
    "# ])\n",
    "# print(y_pred)\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# print(custom_loss(y_true, y_pred).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zQkREYpyeZNP",
    "outputId": "61c71079-d0b7-47c4-9fe0-93beefb3ae56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5108256237659907"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# -np.log(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qec3shDEw30"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    #print(in_id, in_mask, in_segment)\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"sequence_output\")(bert_inputs)\n",
    "    \n",
    "    #print(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)#random drop out to prevent overfitting\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(27, activation='sigmoid')(dense)#Arnobio: 2 for binary class (not sure why) need to change output shpae to reflect number of classes\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    \n",
    "    \n",
    "#     losses = custom_loss#from Joachim's notebook\n",
    "    \n",
    "#     model.compile(loss=losses, optimizer='adam', metrics=['accuracy'])\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[custom_acc_orig_tokens])\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# sparse_categorical_crossentropy works, accuracy function was not working\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#build_model(32)\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szPBKLBLw7nD"
   },
   "source": [
    "###Train the BERT model A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwVb2r25UINh"
   },
   "source": [
    "### Data Preparation\n",
    "Set up data to mimic Joachim's data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkbhuypiTOq8"
   },
   "source": [
    "We first load the process numpy array to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlQCb1KwKZmt"
   },
   "outputs": [],
   "source": [
    "#Here we import the preprocessed dataset from Google Drive. Sync the google drive file with your window so you can select from the drop down menu.\n",
    "\n",
    "\n",
    "# train_token_list1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/train_token_list.npy',encoding='bytes')\n",
    "# train_token_list2=np.array(train_token_list1)\n",
    "# test_token_list1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/test_token_list.npy')\n",
    "# train_input_ids1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/train_input_ids.npy')\n",
    "# test_input_ids1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/test_input_ids.npy')\n",
    "# train_input_masks1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/train_input_masks.npy')\n",
    "# test_input_masks1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/test_input_masks.npy')\n",
    "# train_labels1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/train_labels.npy')\n",
    "# test_labels1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/test_labels.npy')\n",
    "# train_segment_ids1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/train_segment_ids.npy')\n",
    "# test_segment_ids1 = np.load('/gdrive/My Drive/w266_NLP/arrays-multiclass/test_segment_ids.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "03G9KRSOfFB7",
    "outputId": "8ca665ce-aa06-449b-9feb-9e44bc60cb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n",
      "<class 'numpy.str_'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# print(train_token_list1[0][4])\n",
    "# print(train_token_list[0][4])\n",
    "# print(type(train_token_list1[0][4]))\n",
    "# print(type(train_token_list[0][4]))\n",
    "# print(train_token_list1)\n",
    "# print(train_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "hJubB1GFwqeD",
    "outputId": "e6019478-6a71-4492-cd49-a8852a16711d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 17470, 20)\n",
      "(3, 8444, 20)\n"
     ]
    }
   ],
   "source": [
    "#train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_la\n",
    "\n",
    "X_train = np.array([train_input_ids,train_input_masks,train_segment_ids])\n",
    "X_test = np.array([test_input_ids,test_input_masks,test_segment_ids])\n",
    "train_label1=np.array(train_labels)\n",
    "test_label1=np.array(test_labels)\n",
    "\n",
    "print(X_train.shape)#confirm it's (3,73722,20) for training\n",
    "print(X_test.shape) #confirm it's (3, 47683, 20) for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Vx9wMXSX8JmI",
    "outputId": "238adbce-9e41-4254-d20e-51afc65ca9a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[24  0  0  0  0  0  0  0 25 26 26 26 26 26 26 26 26 26 26 26]\n"
     ]
    }
   ],
   "source": [
    "num=101\n",
    "print(X_test[1][num])\n",
    "print(test_label1[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ZVW_81AGxNez",
    "outputId": "a89600f8-5c5a-49cf-bf15-9feba02d003d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(9600, 20)\n",
      "(3200, 20)\n",
      "(3200, 20)\n"
     ]
    }
   ],
   "source": [
    "k_start = 0\n",
    "k_end_train = 9600\n",
    "k_end_dev_start_test = 3200\n",
    "k_end_test = k_end_dev_start_test + 3200\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_dev_k = [X_test[0][k_start:k_end_dev_start_test], X_test[1][k_start:k_end_dev_start_test], \n",
    "                      X_test[2][k_start:k_end_dev_start_test]]\n",
    "bert_inputs_test_k = [X_test[0][k_end_dev_start_test:k_end_test], X_test[1][k_end_dev_start_test:k_end_test], \n",
    "                      X_test[2][k_end_dev_start_test:k_end_test]]\n",
    "\n",
    "bert_train_label = train_label1[k_start:k_end_train]\n",
    "bert_dev_label = test_label1[k_start:k_end_dev_start_test]\n",
    "bert_test_label = test_label1[k_end_dev_start_test:k_end_test]\n",
    "\n",
    "print(len(bert_inputs_train_k))\n",
    "print(bert_train_label.shape)\n",
    "print(bert_dev_label.shape)\n",
    "print(bert_test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYUyiDCXuSOt"
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxbhcJVmj2qn"
   },
   "outputs": [],
   "source": [
    "sess.close()#to close sess if any session is open. If none, you will get an error message whic is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "EZmk2uT2EydW",
    "outputId": "182b73ae-dcf4-4f0d-eeed-df71fe6ac6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 256)    196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 27)     6939        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,135,199\n",
      "Trainable params: 21,467,419\n",
      "Non-trainable params: 87,667,780\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 3200 samples\n",
      "Epoch 1/2\n",
      "9600/9600 [==============================] - 2117s 221ms/sample - loss: 0.5786 - custom_acc_orig_tokens: 0.8795 - val_loss: 0.6762 - val_custom_acc_orig_tokens: 0.9038\n",
      "Epoch 2/2\n",
      "9600/9600 [==============================] - 2126s 221ms/sample - loss: 0.5161 - custom_acc_orig_tokens: 0.7657 - val_loss: 0.5706 - val_custom_acc_orig_tokens: 0.7521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7712335cf8>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "# # #https://stackoverflow.com/questions/34001922/failedpreconditionerror-attempting-to-use-uninitialized-in-tensorflow\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model = build_model(max_seq_length)\n",
    "\n",
    "# \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "# model.fit(\n",
    "#     bert_inputs_train_k, \n",
    "#     {\"ner\": labels_train_k },\n",
    "#     validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "#     epochs=8,\n",
    "#     batch_size=32#,\n",
    "#     #callbacks=[tensorboard]\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    bert_train_label,\n",
    "    validation_data=(bert_inputs_dev_k,bert_dev_label),\n",
    "    epochs=2,\n",
    "    batch_size=32#if we change this to input dimension then we solve the 32 problem. \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYkoe9l1uY0f"
   },
   "source": [
    "### Save Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNLjARLCuqCu"
   },
   "outputs": [],
   "source": [
    "# download model to local machine\n",
    "from google.colab import files\n",
    "\n",
    "# download train arrays\n",
    "files.download('modelA.h5')\n",
    "files.download('modelA_weights.h5')\n",
    "\n",
    "# files manually uploaded to drive (models folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGx1GqkIwbl5"
   },
   "source": [
    "###Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZyiV9TIwifs"
   },
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_test_k, \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8zGtV8IBvJLk",
    "outputId": "87a6d803-9bd1-4990-8e12-a57e85cc5f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "#Save result for future reference.\n",
    "%cd ..\n",
    "np.save('result_modelA1',result)\n",
    "\n",
    "from google.colab import files\n",
    "files.download('result_modelA1.npy')\n",
    "np.save('test_label_modelA1',bert_test_label)\n",
    "files.download('test_label_modelA1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "hNJY3QyIyEdG",
    "outputId": "04fec738-070d-4d3a-bd32-d5e06565b9de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of result is: (3200, 20, 27)\n",
      "The shape of test label is: (3200, 20)\n",
      "The predicted label is: [24  0  0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0  0 25]\n",
      "The actual label is: [24  0  0  0  0  0  0  0  0  0  0  0  1  0  1  1  0  1  1 25]\n",
      "The input mask is: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "num=66\n",
    "print(\"The shape of result is:\",result.shape)\n",
    "print(\"The shape of test label is:\", bert_test_label.shape)\n",
    "#print(result[2][0])\n",
    "pre_np=np.argmax(result, axis=2)\n",
    "pre_np.shape\n",
    "print(\"The predicted label is:\", pre_np[num])\n",
    "print(\"The actual label is:\", bert_test_label[num])\n",
    "print(\"The input mask is:\",X_test[1][3200+num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWzf9IKzoiNs"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# pred_np=np.argmax(result, axis=2)\n",
    "# print(pred_np.shape)\n",
    "# print(pred_np.shape[0])\n",
    "# test_np=bert_test_label\n",
    "# print(len(test_np))\n",
    "\n",
    "# pred_label=[]\n",
    "# test_label=[]\n",
    "\n",
    "# for i in range(pred_np.shape[0]):\n",
    "#   pred_label.extend(pred_np[i])\n",
    "\n",
    "# for i in range(len(test_np)):\n",
    "#   test_label.extend(test_np[i])\n",
    "  \n",
    "  \n",
    "# print(len(pred_label), len(test_label))\n",
    "\n",
    "# print(classification_report(test_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XI3oVkqJAQdf"
   },
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in bert_test_label for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 24:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "bgFOxW44Cr_z",
    "outputId": "23767b12-4446-4133-a0f9-2c59924b0de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34510\n",
      "34510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87     28362\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00       120\n",
      "          25       0.00      0.00      0.00        64\n",
      "          26       0.00      0.00      0.00      5964\n",
      "\n",
      "    accuracy                           0.76     34510\n",
      "   macro avg       0.04      0.05      0.05     34510\n",
      "weighted avg       0.67      0.76      0.71     34510\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(clean_preds))\n",
    "print(len(clean_labels))\n",
    "print(classification_report(clean_preds, clean_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bISKCaCAAqFQ"
   },
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "FTMxFEOdA_iu",
    "outputId": "a047078a-698b-4d19-c3a5-04eaa066278e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28362,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,   120,    64,  5964])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sbbVKdrDBBUv",
    "outputId": "1fa1edb0-5f18-4e44-8554-300a759572c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26319     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    118    64  5732]\n",
      " [  909     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0   183]\n",
      " [  401     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      1     0    13]\n",
      " [  143     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     8]\n",
      " [  163     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     7]\n",
      " [   49     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     5]\n",
      " [  117     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     2]\n",
      " [   45     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     1]\n",
      " [   16     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   19     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     4]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   55     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     4]\n",
      " [   59     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   28     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      1     0     2]\n",
      " [   23     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [   15     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     1]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     2]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#print (cm)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "vMXaH62HBuZb",
    "outputId": "4be8ec2e-05f6-42c8-c0b9-52aeafde453a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26319     0     0     0     0     0     0     0     0     0     0]\n",
      " [  909     0     0     0     0     0     0     0     0     0     0]\n",
      " [  401     0     0     0     0     0     0     0     0     0     0]\n",
      " [  143     0     0     0     0     0     0     0     0     0     0]\n",
      " [  163     0     0     0     0     0     0     0     0     0     0]\n",
      " [   49     0     0     0     0     0     0     0     0     0     0]\n",
      " [  117     0     0     0     0     0     0     0     0     0     0]\n",
      " [   45     0     0     0     0     0     0     0     0     0     0]\n",
      " [   16     0     0     0     0     0     0     0     0     0     0]\n",
      " [   19     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "cm_most = cm[[0,1,2,3,4,5,6,7,8,9,10],:] [:, [0,1,2,3,4,5,6,7,8,9,10]]\n",
    "print(cm_most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JF6j1Qcv00C3",
    "outputId": "03d8a125-6505-4b43-a52b-27144ce2bdc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   756   972     1     4   746   339  1956   592    65     1     0\n",
      "    127    15  2611     0   627  1295    33   128  1592    29  3561     6\n",
      "     24 16752     1]\n",
      " [    0    23    37     0     0    38    32    86    45     8     0     0\n",
      "      7     0    36     0    71    12     0     8    39     0   114     0\n",
      "      6   530     0]\n",
      " [    0    23     2     0     0    28    11    66     2     0     0     0\n",
      "      5     0    15     0     8    35     1     1    29     0    33     0\n",
      "      0   156     0]\n",
      " [    0     4     9     0     0     5     1     8     1     0     0     0\n",
      "      1     0    13     0     0     5     0     0     1     0    16     0\n",
      "      0    87     0]\n",
      " [    0    19     1     0     0     9     1    42     1     0     0     0\n",
      "      4     0     9     0     2     5     0     0    10     1    12     0\n",
      "      0    54     0]\n",
      " [    0     5     0     0     0     0     1     2     5     0     0     0\n",
      "      1     0     1     0     2     2     0     0     0     0    13     0\n",
      "      0    22     0]\n",
      " [    0     0     4     0     0     0     2    16     5     0     0     0\n",
      "      0     0    11     0     0     5     0     0     8     0     9     0\n",
      "      0    59     0]\n",
      " [    0     3     0     0     0     1     0     1     0     2     0     0\n",
      "      0     0     3     0     1     1     0     0     8     0     3     0\n",
      "      0    23     0]\n",
      " [    0     0     2     0     0     0     0     0     0     0     0     0\n",
      "      2     0     1     0     0     0     0     0     2     0     1     0\n",
      "      0     8     0]\n",
      " [    0     2     1     0     0     0     0     2     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     2     0\n",
      "      0    16     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     1     3     0     0     0     5     3     0     0     0     0\n",
      "      0     0     1     0     0     1     0     0     1     0     1     0\n",
      "      0    43     0]\n",
      " [    0     1     2     0     0     8     0    11     1     0     0     0\n",
      "      0     0     4     0     1     1     0     0     4     0     8     0\n",
      "      0    18     0]\n",
      " [    0     0     0     0     0     1     0     0     0     0     0     0\n",
      "      1     0     3     0     1     4     0     0     0     0     1     0\n",
      "      0    20     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0    23     0]\n",
      " [    0     1     0     0     0     0     0     1     0     0     0     0\n",
      "      0     0     1     0     1     1     0     0     3     0     1     0\n",
      "      0     7     0]\n",
      " [    0     0     0     0     0     1     0     0     0     0     0     0\n",
      "      0     0     1     0     0     0     0     0     1     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     1     1     0\n",
      "      0  3198     0]\n",
      " [    0     0    13     0    17     1     3    14    27    93     1     0\n",
      "     16   165    24     0    39   470     0     1    28     6    68     0\n",
      "      1  2212     1]\n",
      " [    0   801  1880     0     0    65   507  1822    76     3     0     0\n",
      "      8     3   246     0  1302   176   159   240   233    14  5929     0\n",
      "      9  9617     0]]\n"
     ]
    }
   ],
   "source": [
    "con=confusion_matrix(test_label, pred_label)\n",
    "print(con)\n",
    "# for i in range(len(con)):#print it one by one so it's easier to see\n",
    "#   print(con[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsHjixVi05dn"
   },
   "outputs": [],
   "source": [
    "con=confusion_matrix(test_label, pred_label)\n",
    "for i in range(len(con)):#print it one by one so it's easier to see\n",
    "  print(con[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZWOm0HjyrFJ"
   },
   "source": [
    "## Building out model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsDtfofYyuQP"
   },
   "outputs": [],
   "source": [
    "# changes to the model\n",
    "\n",
    "# changed fine tune layers from 3 to 10\n",
    "n_fine_tune_layers = 10\n",
    "\n",
    "#BERT_MODEL_HUB\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=n_fine_tune_layers,\n",
    "        pooling=\"sequence_output\",\n",
    "        bert_path=BERT_MODEL_HUB,\n",
    "        #bert_path=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        #             trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "        ]\n",
    "        \n",
    "        trainable_layers = []\n",
    "        #         else:\n",
    "        #             raise NameError(\n",
    "        #                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "        #             )\n",
    "\n",
    "                # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"pooled_output\"\n",
    "        #             ]\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        #             result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"sequence_output\"\n",
    "        #             ]\n",
    "\n",
    "        #             mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        #             masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "        #                     tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "        #             input_mask = tf.cast(input_mask, tf.float32)\n",
    "        #             pooled = masked_reduce_mean(result, input_mask)\n",
    "        #         else:\n",
    "        #             raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "                \n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "        \n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3GWndmEz5Vq"
   },
   "source": [
    "### Train the BERT model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgybfuSuzgm4"
   },
   "outputs": [],
   "source": [
    "# no changes to custom loss or custom accuracy functions\n",
    "\n",
    "# changed learning rate from 0.1 to 0.08\n",
    "learning_rate = 0.08\n",
    "\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    #print(in_id, in_mask, in_segment)\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=n_fine_tune_layers, pooling=\"sequence_output\")(bert_inputs)\n",
    "    \n",
    "    #print(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=learning_rate)(dense)#random drop out to prevent overfitting\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(27, activation='sigmoid')(dense)#Arnobio: 2 for binary class (not sure why) need to change output shpae to reflect number of classes\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    \n",
    "    \n",
    "#     losses = custom_loss#from Joachim's notebook\n",
    "    \n",
    "#     model.compile(loss=losses, optimizer='adam', metrics=['accuracy'])\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[custom_acc_orig_tokens])\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# sparse_categorical_crossentropy works, accuracy function was not working\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#build_model(32)\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaYQzfVqz_r1"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uLroQtNfzn79",
    "outputId": "7ff514b3-c006-43fb-cc17-63d9a8d41489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 73744, 20)\n"
     ]
    }
   ],
   "source": [
    "#train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_la\n",
    "\n",
    "X_train = np.array([train_input_ids,train_input_masks,train_segment_ids])\n",
    "X_test = np.array([test_input_ids,test_input_masks,test_segment_ids])\n",
    "train_label1=np.array(train_labels)\n",
    "test_label1=np.array(test_labels)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jBYRlbWK0FDq",
    "outputId": "fa7b09c1-cc6b-4f2e-f503-41772532666d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(6400, 20)\n",
      "(6400, 20)\n"
     ]
    }
   ],
   "source": [
    "# doubled the amount of data\n",
    "\n",
    "k_start = 0\n",
    "k_end_train = 6400\n",
    "k_end_dev_start_test = 6400\n",
    "k_end_test = k_end_dev_start_test + 6400\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "\n",
    "bert_inputs_dev_k = [X_test[0][k_start:k_end_dev_start_test], X_test[1][k_start:k_end_dev_start_test], \n",
    "                      X_test[2][k_start:k_end_dev_start_test]]\n",
    "\n",
    "bert_inputs_test_k = [X_test[0][k_end_dev_start_test:k_end_test], X_test[1][k_end_dev_start_test:k_end_test], \n",
    "                      X_test[2][k_end_dev_start_test:k_end_test]]\n",
    "\n",
    "bert_train_label = train_label1[k_start:k_end_train]\n",
    "bert_dev_label = test_label1[k_start:k_end_dev_start_test]\n",
    "bert_test_label = test_label1[k_end_dev_start_test:k_end_test]\n",
    "\n",
    "print(len(bert_inputs_train_k))\n",
    "print(bert_train_label.shape)\n",
    "print(bert_test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6rC0xKOGv2P"
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "M6XSM7Ld0Pqq",
    "outputId": "439b4252-2cd9-4eef-957a-31cd2eb47bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_10 (BertLayer)       (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, None, 256)    196864      bert_layer_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, 256)    0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, None, 27)     6939        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,135,199\n",
      "Trainable params: 21,467,419\n",
      "Non-trainable params: 87,667,780\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 6400 samples\n",
      "Epoch 1/8\n",
      "6400/6400 [==============================] - 641s 100ms/sample - loss: 0.6939 - custom_acc_orig_tokens: 0.7561 - val_loss: 1.0482 - val_custom_acc_orig_tokens: 0.1019\n",
      "Epoch 2/8\n",
      "6400/6400 [==============================] - 630s 98ms/sample - loss: 1.1137 - custom_acc_orig_tokens: 0.8779 - val_loss: 1.0684 - val_custom_acc_orig_tokens: 0.9290\n",
      "Epoch 3/8\n",
      "6400/6400 [==============================] - 634s 99ms/sample - loss: 1.0199 - custom_acc_orig_tokens: 0.9267 - val_loss: 0.9215 - val_custom_acc_orig_tokens: 0.9289\n",
      "Epoch 4/8\n",
      "6400/6400 [==============================] - 634s 99ms/sample - loss: 1.1155 - custom_acc_orig_tokens: 0.9264 - val_loss: 1.0511 - val_custom_acc_orig_tokens: 0.9288\n",
      "Epoch 5/8\n",
      "6400/6400 [==============================] - 631s 99ms/sample - loss: 1.0358 - custom_acc_orig_tokens: 0.9203 - val_loss: 1.1538 - val_custom_acc_orig_tokens: 0.9274\n",
      "Epoch 6/8\n",
      "6400/6400 [==============================] - 630s 98ms/sample - loss: 1.0136 - custom_acc_orig_tokens: 0.9256 - val_loss: 0.9115 - val_custom_acc_orig_tokens: 0.9288\n",
      "Epoch 7/8\n",
      "6400/6400 [==============================] - 632s 99ms/sample - loss: 0.9136 - custom_acc_orig_tokens: 0.8236 - val_loss: 0.8723 - val_custom_acc_orig_tokens: 0.6938\n",
      "Epoch 8/8\n",
      "6400/6400 [==============================] - 630s 98ms/sample - loss: 0.8203 - custom_acc_orig_tokens: 0.7473 - val_loss: 0.6698 - val_custom_acc_orig_tokens: 0.8079\n"
     ]
    }
   ],
   "source": [
    "#changed to 8 epochs\n",
    "\n",
    "#keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "# # #https://stackoverflow.com/questions/34001922/failedpreconditionerror-attempting-to-use-uninitialized-in-tensorflow\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.close()\n",
    "\n",
    "model = build_model(max_seq_length)\n",
    "\n",
    "# \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "# model.fit(\n",
    "#     bert_inputs_train_k, \n",
    "#     {\"ner\": labels_train_k },\n",
    "#     validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "#     epochs=8,\n",
    "#     batch_size=32#,\n",
    "#     #callbacks=[tensorboard]\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    bert_train_label,\n",
    "    validation_data=(bert_inputs_test_k,bert_test_label),\n",
    "    epochs=epochs,\n",
    "    batch_size=32#if we change this to input dimension then we solve the 32 problem. \n",
    ")\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKeTCwCjGpFZ"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IF6i_adkG4G5"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "sess.close()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model.save('modelB.h5')    \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEQ3WllWG6vn"
   },
   "outputs": [],
   "source": [
    "# save weights for the model\n",
    "sess.close()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model.save_weights('modelB_weights.h5')\n",
    "sess.close()\n",
    "# reference for saving and downloading models https://stackoverflow.com/questions/48924165/google-colaboratory-weight-download-export-saved-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Cm3Z2DDG_eG"
   },
   "outputs": [],
   "source": [
    "# download model to local machine\n",
    "from google.colab import files\n",
    "\n",
    "# download train arrays\n",
    "files.download('modelB.h5')\n",
    "files.download('modelB_weights.h5')\n",
    "\n",
    "# files manually uploaded to drive (models folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9AwBiJrHBob"
   },
   "source": [
    "## Building out model C (all data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTHO8ez4HK3t"
   },
   "source": [
    "### Train BERT Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrATz45sHNvV"
   },
   "outputs": [],
   "source": [
    "# changes to the model\n",
    "\n",
    "# kept 10 fine tune layers as in model B\n",
    "n_fine_tune_layers = 10\n",
    "\n",
    "#BERT_MODEL_HUB\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=n_fine_tune_layers,\n",
    "        pooling=\"sequence_output\",\n",
    "        bert_path=BERT_MODEL_HUB,\n",
    "        #bert_path=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        #             trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "        ]\n",
    "        \n",
    "        trainable_layers = []\n",
    "        #         else:\n",
    "        #             raise NameError(\n",
    "        #                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "        #             )\n",
    "\n",
    "                # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"pooled_output\"\n",
    "        #             ]\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        #             result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"sequence_output\"\n",
    "        #             ]\n",
    "\n",
    "        #             mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        #             masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "        #                     tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "        #             input_mask = tf.cast(input_mask, tf.float32)\n",
    "        #             pooled = masked_reduce_mean(result, input_mask)\n",
    "        #         else:\n",
    "        #             raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "                \n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "        \n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RP8fgvdXHUoW"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGzasE0zHZ0P"
   },
   "outputs": [],
   "source": [
    "#train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_la\n",
    "\n",
    "X_train = np.array([train_input_ids,train_input_masks,train_segment_ids])\n",
    "X_test = np.array([test_input_ids,test_input_masks,test_segment_ids])\n",
    "train_label1=np.array(train_labels)\n",
    "test_label1=np.array(test_labels)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irGPvvT6Hced"
   },
   "outputs": [],
   "source": [
    "# doubled the amount of data\n",
    "\n",
    "k_start = 0\n",
    "k_end_train = 6400\n",
    "k_end_dev_start_test = 6400\n",
    "k_end_test = k_end_dev_start_test + 6400\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "\n",
    "bert_inputs_dev_k = [X_test[0][k_start:k_end_dev_start_test], X_test[1][k_start:k_end_dev_start_test], \n",
    "                      X_test[2][k_start:k_end_dev_start_test]]\n",
    "\n",
    "bert_inputs_test_k = [X_test[0][k_end_dev_start_test:k_end_test], X_test[1][k_end_dev_start_test:k_end_test], \n",
    "                      X_test[2][k_end_dev_start_test:k_end_test]]\n",
    "\n",
    "bert_train_label = train_label1[k_start:k_end_train]\n",
    "bert_dev_label = test_label1[k_start:k_end_dev_start_test]\n",
    "bert_test_label = test_label1[k_end_dev_start_test:k_end_test]\n",
    "\n",
    "print(len(bert_inputs_train_k))\n",
    "print(bert_train_label.shape)\n",
    "print(bert_test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6ZbH65nHWx1"
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DLJhqbrHetV"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5O3IhklHi6a"
   },
   "outputs": [],
   "source": [
    "#changed to 8 epochs\n",
    "\n",
    "#keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "# # #https://stackoverflow.com/questions/34001922/failedpreconditionerror-attempting-to-use-uninitialized-in-tensorflow\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.close()\n",
    "\n",
    "model = build_model(max_seq_length)\n",
    "\n",
    "# \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "# model.fit(\n",
    "#     bert_inputs_train_k, \n",
    "#     {\"ner\": labels_train_k },\n",
    "#     validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "#     epochs=8,\n",
    "#     batch_size=32#,\n",
    "#     #callbacks=[tensorboard]\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    bert_train_label,\n",
    "    validation_data=(bert_inputs_test_k,bert_test_label),\n",
    "    epochs=epochs,\n",
    "    batch_size=32#if we change this to input dimension then we solve the 32 problem. \n",
    ")\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmyrFk5FHYOs"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDVY0PwMHnmw"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "sess.close()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model.save('modelC.h5')    \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SspXIR2HoIs"
   },
   "outputs": [],
   "source": [
    "# save weights for the model\n",
    "sess.close()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize_vars(sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model.save_weights('modelC_weights.h5')\n",
    "sess.close()\n",
    "# reference for saving and downloading models https://stackoverflow.com/questions/48924165/google-colaboratory-weight-download-export-saved-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1C5QeCZHqqU"
   },
   "outputs": [],
   "source": [
    "# download model to local machine\n",
    "from google.colab import files\n",
    "\n",
    "# download train arrays\n",
    "files.download('modelC.h5')\n",
    "files.download('modelC_weights.h5')\n",
    "\n",
    "# files manually uploaded to drive (models folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIL1kUPOQE3-"
   },
   "source": [
    "#Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQPshVVxJhB3"
   },
   "source": [
    "##Model Predict and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "VuECrTHSDMCr",
    "outputId": "f9258e8c-7f41-40fd-8c0c-c772c2d428db"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-44d6306eb418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#bert_inputs_infer = [X_test[0][0:64], X_test[1][0:64], X_test[2][0:64]]\n",
    "#test_input_ids.shape, test_input_masks.shape, test_token_list.shape, test_segment_ids.shape, test_labels.shape)\n",
    "#X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "#print(test_input_ids[0:320])\n",
    "\n",
    "#bert_inputs_infer=[test_input_ids[0:32], test_input_masks[0:32], test_segment_ids[0:32]]\n",
    "\n",
    "#print(len(bert_inputs_infer[0]))\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_test_k, \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7QW7AdkJYbx"
   },
   "outputs": [],
   "source": [
    "# print(type(result))\n",
    "# print(result.shape)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "CNtc-pGxi3wZ",
    "outputId": "54833746-fb35-4720-db62-d69de976b528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of 0 in test label is: 994\n",
      "total number of token to be labelled is 1024\n",
      "accuracy if alwasy guess 0 is: 0.970703125\n",
      "total number of when preict and test is the same is: 945\n",
      "accuracy is 0.9228515625\n"
     ]
    }
   ],
   "source": [
    "# #Convert all probabiliyt less than 0.5 to 1 else 0\n",
    "# for i in range(32):\n",
    "#   for j in range(32):\n",
    "#     if result[i][j]<=0.5:\n",
    "#       result[i][j]=1\n",
    "#     else:\n",
    "#       result[i][j]=0\n",
    "\n",
    "# zero_count=0\n",
    "# for i in range(32):\n",
    "#   for j in range(32):\n",
    "#     if test_label[i][j]==0:\n",
    "#       zero_count+=1\n",
    "      \n",
    "# count=0\n",
    "# for i in range(32):\n",
    "#   for j in range(32):\n",
    "#     if result[i][j]==test_label[i][j]:\n",
    "#       count+=1\n",
    "    \n",
    "    \n",
    "     \n",
    "# #print (result[0][1]==test_label[0][1])\n",
    "      \n",
    "# print(\"total number of 0 in test label is:\", zero_count)\n",
    "# print(\"total number of token to be labelled is\", total)\n",
    "# print(\"accuracy if alwasy guess 0 is:\", zero_count/total)\n",
    "\n",
    "# print(\"total number of when preict and test is the same is:\", count)\n",
    "# total=32*32\n",
    "\n",
    "\n",
    "\n",
    "# print(\"accuracy is\", count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "jMzen2GMjhx-",
    "outputId": "d1f223c2-8d17-48ac-c49e-3091387e7948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[0.525739   0.17723814 0.51883465 0.61479783 0.3393644  0.61199903\n",
      " 0.6204618  0.61572665 0.7662282  0.42423853 0.4052503  0.44994062\n",
      " 0.47312936 0.6477987  0.5720914  0.6492477  0.4279942  0.6344584\n",
      " 0.28458297 0.52113175 0.5569579  0.48475057 0.5200761  0.29491848\n",
      " 0.7383041  0.5899198  0.6237016 ]\n"
     ]
    }
   ],
   "source": [
    "print(len(result[0]))\n",
    "print(result[2])#len is 27 represents the probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "CdcSVu-QJl6E",
    "outputId": "f94a0af7-6d90-4f91-910d-da2546cf17c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33326754 0.30363172 0.3720487  0.6436627  0.3298968  0.44187316\n",
      " 0.4364891  0.5344843  0.59936625 0.44336876 0.42019337 0.47927517\n",
      " 0.5198763  0.6234932  0.61675876 0.6559623  0.40592694 0.48301476\n",
      " 0.54483455 0.43970555 0.4094195  0.5696674  0.41309303 0.32302675\n",
      " 0.6597335  0.37658548 0.5924104 ]\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "#print(np.argmax(result, axis=1))\n",
    "x=np.argmax(result, axis=2)\n",
    "print(result[2][3])\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGeGerhz7C7e"
   },
   "source": [
    "##Classification Report\n",
    "\n",
    "https://stackoverflow.com/questions/1783653/computing-precision-and-recall-in-named-entity-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "2iS3jKLb9ZlU",
    "outputId": "d0812f49-47df-4843-aa35-2f3169e1643a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a4959f9b4ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "qFPZDnPWCmT8",
    "outputId": "afb44a82-b4fd-4bdf-ec13-be6f8151788a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "64\n",
      "64\n",
      "1280 1280\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       732\n",
      "           1       0.00      0.00      0.00        17\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.20      0.00         5\n",
      "           8       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00        64\n",
      "          25       0.01      0.03      0.02        64\n",
      "          26       0.00      0.00      0.00       388\n",
      "\n",
      "    accuracy                           0.00      1280\n",
      "   macro avg       0.00      0.01      0.00      1280\n",
      "weighted avg       0.00      0.00      0.00      1280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred_np=np.argmax(result, axis=2)\n",
    "print(pred_np.shape)\n",
    "print(pred_np.shape[0])\n",
    "test_np=bert_test_label[0:64]\n",
    "print(len(test_np))\n",
    "\n",
    "pred_label=[]\n",
    "test_label=[]\n",
    "\n",
    "for i in range(pred_np.shape[0]):\n",
    "  pred_label.extend(pred_np[i])\n",
    "\n",
    "for i in range(len(test_np)):\n",
    "  test_label.extend(test_np[i])\n",
    "  \n",
    "  \n",
    "print(len(pred_label), len(test_label))\n",
    "\n",
    "print(classification_report(test_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJp7gDXM6xpX"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(result, axis=2)[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuQai8Y863dX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "eVm-Lx6Y5VNR",
    "outputId": "55262d68-cd0d-432d-9e9f-1669c049418a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "24\n",
      "6\n",
      "15\n",
      "24\n",
      "10\n",
      "24\n",
      "24\n",
      "24\n",
      "6\n",
      "15\n",
      "24\n",
      "6\n",
      "24\n",
      "21\n",
      "24\n",
      "24\n",
      "21\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "for i in result[0]:\n",
    "  print(np.argmax(i))\n",
    "#np.argmax(result[0][1])\n",
    "#np.argmax(result[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZLVK2Krz6Rvv",
    "outputId": "0ea34cb3-4c5c-44df-ae9f-8bfe9815f15e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 24,  6, 15, 24, 10, 24, 24, 24,  6, 15, 24,  6, 24, 21, 24, 24,\n",
       "       21, 24, 24])"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result, axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "kTWZzS-t5o2H",
    "outputId": "11939cd9-4120-4b71-f3d6-67271a5af824"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41188538, 0.28872663, 0.38126117, 0.47752753, 0.3996588 ,\n",
       "       0.42883107, 0.69209856, 0.23030123, 0.2783481 , 0.3330383 ,\n",
       "       0.7107179 , 0.21297577, 0.66454923, 0.4015806 , 0.31005138,\n",
       "       0.77361715, 0.589921  , 0.5715536 , 0.34632716, 0.53967047,\n",
       "       0.56555796, 0.3240528 , 0.46150875, 0.350986  , 0.76940316,\n",
       "       0.73595166, 0.6687447 ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42Fi8rZgJOyq"
   },
   "source": [
    "##Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "id": "RGYnyzZ0Jq9o",
    "outputId": "bcce0534-fdb1-4c3d-cba5-bed274927729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   0  17 105   0  35  29  90  13   4  16   0   5  96  72 162   2  22\n",
      "   1  11  15   1   3  45   6  67]\n",
      "[0 0 1 3 0 4 0 0 0 0 2 1 0 0 1 4 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0  0  0  0  0  0  5  6 29  0  0  0  0  0  0  3  0  0  0  0  0  0  0 17\n",
      "  2  2]\n",
      "[ 1  0  0  5  1  0  1  1  0  0  1  0  0  0  5 42  0  2  0  0  3  0  0  0\n",
      "  2  0]\n",
      "[  1   0   2  88   0   9  28 254  23   1   1   0   2 207  53 232   5  19\n",
      "  36   2  45   0   1  14   0  47]\n"
     ]
    }
   ],
   "source": [
    "con=confusion_matrix(test_label, pred_label)\n",
    "for i in range(len(con)):#print it one by one so it's easier to see\n",
    "  print(con[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNfCqvfPUDLY"
   },
   "outputs": [],
   "source": [
    "#generate_encoded_data(train_filelist[5:6])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "De-Identification Using Bert (Multi-Class)-ModelA, Shared.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
