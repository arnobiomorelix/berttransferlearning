{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4Hw4aOBsECR"
   },
   "source": [
    "# Setup\n",
    "based on https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=191zq3ZErihP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "Q-h6Nk0WsICS",
    "outputId": "1a438550-6891-4e60-dfb8-8095306e7d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU address is grpc://10.108.13.194:8470\n",
      "TPU devices:\n",
      "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 861201113876687645),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7509863777384350548),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10957324747785543829),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16010609198301123736),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15642925627654617544),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 202491004858483873),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7185590080412384499),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9450033752344641433),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5937089048049227403),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16704279036719683882),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1360056868896268412)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 06:36:28.714955 140462451693440 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "print('TPU address is', TPU_ADDRESS)\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "with tf.Session(TPU_ADDRESS) as session:\n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(session.list_devices())\n",
    "\n",
    "  # Upload credentials to TPU.\n",
    "  with open('/content/adc.json', 'r') as f:\n",
    "    auth_info = json.load(f)\n",
    "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "  # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8Dxp0OwI_9X"
   },
   "source": [
    "# Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eqCtk6_OBt3"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SFwSCJIsJFkB",
    "outputId": "4ae1385e-5993-4e84-cb81-42eb3c7de6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#This file is written to ingest data from i2b2\n",
    "#below are the requried library\n",
    "from xml.dom import minidom # need this to read xlm files\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import DataFrame\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt') #this package needs to be downloaded separately\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tSyXkHRwdPZ"
   },
   "source": [
    "### GDrive Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsap3HvwNtrA"
   },
   "source": [
    "We first mount the google drive containing the training and test fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0dp7AABnNcce",
    "outputId": "778685a3-370c-486a-96bb-0a6eedb9d3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive # this sets the file path to your personal google drive. You will need to enter the authorization code each time. \n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCX9KxgT2zy7"
   },
   "source": [
    "## Creating List of Files to be Ingested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hqr9iRr1N1h0"
   },
   "source": [
    "We then created two list - each contains the list of the file names for training and testing.\n",
    "\n",
    "1.   train_filelist = 790 EHR records\n",
    "2.   test_filelist=514 EHR records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "lMonhsbPNgDi",
    "outputId": "2ef2193e-8937-4632-cfdb-d1ef17402b8f"
   },
   "outputs": [],
   "source": [
    "# data processing created with the help of teaching assistant Sudha Subramanian, who previously worked with the same dataset\n",
    "\n",
    "train_filelist=[]\n",
    "\n",
    "for file in os.listdir('/gdrive/My Drive/w266_NLP/training-PHI'):#set your file path here\n",
    "  filename = os.fsdecode(os.fsencode('/gdrive/My Drive/w266_NLP/training-PHI/'+file))\n",
    "  if filename.endswith( ('.xml') ): # select xml files\n",
    "    train_filelist.append(filename)\n",
    "\n",
    "print(\"There are {} training file\".format(len(train_filelist))) #check that the number of training file is 790 records for 178 patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "MpT-K7y1NlhM",
    "outputId": "3d40cbab-fc17-442a-b69b-047f337d0f4d"
   },
   "outputs": [],
   "source": [
    "test_filelist=[]\n",
    "\n",
    "for file in os.listdir('/gdrive/My Drive/w266_NLP/test-PHI'):#set your file path here\n",
    "  filename = os.fsdecode(os.fsencode('/gdrive/My Drive/w266_NLP/test-PHI/'+file))\n",
    "  if filename.endswith( ('.xml') ): # select xml files\n",
    "    test_filelist.append(filename)\n",
    "\n",
    "print(\"There are {} test file\".format(len(test_filelist))) #check that the number of test file is 514 records for 178 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHkiAyKCOOfY"
   },
   "source": [
    "# Process Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46iA8aDpOecR"
   },
   "source": [
    "The tag generator process the annotation into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-mR_51sNr7O"
   },
   "outputs": [],
   "source": [
    "def tag_generator(file):\n",
    "  '''The function extract the tags from the EHR record and turn them into pd dataframe'''\n",
    "  tree = ET.parse(file)\n",
    "  root=tree.getroot()\n",
    "  \n",
    "  PHI_category=['NAME','PROFESSION','LOCATION','AGE','DATE','CONTACT','ID']# Here are the seven PHI category defined by i2b2\n",
    "  #PHI_category=[category]\n",
    "  tag_list=[]#An empty list to hold all dictionary items\n",
    "  for category in PHI_category:\n",
    "    for tag in root.iter():\n",
    "      if tag.tag==category:#skip if a specific tag is not found\n",
    "          tag.attrib['Category']=category #add a column on category\n",
    "          tag.attrib['File']=file[len(file)-10:len(file)-4] # add a column to indicate file name\n",
    "          tag_list.append(tag.attrib)\n",
    "  temp_df=pd.DataFrame(tag_list)\n",
    "      \n",
    "  return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZMzXvl6iPix"
   },
   "outputs": [],
   "source": [
    "def note_generator(file):\n",
    "  #'''This function breakdown inidividaul EHR text note into sentences, divided by new line and period'''\n",
    "    tree = ET.ElementTree(file=file)\n",
    "    root = tree.getroot()\n",
    "    all_notes = []\n",
    "\n",
    "    text = root.find('TEXT').text\n",
    "    sentences = [sent.split('\\n') for sent in sent_tokenize(text) if sent!='\\n']\n",
    "    \n",
    "\n",
    "    for text in sentences:#this part ignore empty lines\n",
    "        for sub_item in text:\n",
    "            if sub_item.replace(' ','') != '':\n",
    "                all_notes.append(sub_item)    \n",
    "    \n",
    "    return all_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REVnrHt7gMsN"
   },
   "source": [
    "### Install Bert tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "tEc2dLtQ5HPt",
    "outputId": "870d8a1d-a610-441b-f714-f8777276cbd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 06:38:35.420527 140462451693440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow    # this replaces the bert github clone\n",
    "!pip install keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vLTuU4VQ4geo",
    "outputId": "316530d7-4fd3-4ba4-ce59-5e1105d5896f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 06:41:18.301390 140462451693440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT(PW:cased?)\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xN-hCdU7qF9"
   },
   "outputs": [],
   "source": [
    "def sentence_encoding(file):#this function is looped within the token_annotator function\n",
    "  \n",
    "  sentence_list=note_generator(file) #generate a list of sentences from tex\n",
    "  \n",
    "  df=tag_generator(file)\n",
    "  text_list=df['text'].tolist() #generate a list of tag \"TEXT\"\n",
    "  type_list=df['TYPE'].tolist() #generate a list of tag \"type\"\n",
    "  category_list=df['Category'].tolist() #generate a list of tag \"category\"\n",
    "  \n",
    "  processed_sentence=[]\n",
    "  processed_text=[]\n",
    "  processed_type=[]\n",
    "  processed_category=[]\n",
    "  \n",
    "  def findWholeWord(w):#this function finds a word within a string broken down by regular expression (case sensitive)\n",
    "    return re.compile(r'\\b({0})\\b'.format(w)).search\n",
    "  \n",
    "  for sentence in sentence_list:\n",
    "     for text in text_list:\n",
    "        if findWholeWord(text)(sentence)!=None:\n",
    "          processed_sentence.append(sentence)\n",
    "          processed_text.append(text)\n",
    "          processed_type.append(type_list[text_list.index(text)])\n",
    "          processed_category.append(category_list[text_list.index(text)])\n",
    "\n",
    "  \n",
    "  temp_df=pd.DataFrame({'Sentence':processed_sentence, 'Word':processed_text, 'Type':processed_type, 'Category':processed_category})\n",
    "  df = temp_df.drop_duplicates()\n",
    "        \n",
    "  return df\n",
    "  #return sentence_list, text_list, type_list, category_list\n",
    "  #return processed_sentence, processed_text, processed_type\n",
    "\n",
    "# sentence_encoding(train_filelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQon1Y3AEfwI"
   },
   "outputs": [],
   "source": [
    "def token_annotator(file):\n",
    "  \n",
    "  temp_df=sentence_encoding(file)#take the data frame and turn them into individual lists\n",
    "  \n",
    "  type_list=temp_df['Type'].tolist()\n",
    "  temp_sentence_list=temp_df['Sentence'].tolist()\n",
    "  word_list=temp_df['Word'].tolist()\n",
    "  temp_unique_sentence_list=set(temp_sentence_list)\n",
    "  sentence_list=list(temp_unique_sentence_list) #take out duplicate sentences\n",
    "  \n",
    "  tokenized_word=[] #separate individual text into words (e.g, Mia E. Tapia to \"Mia\",\"E.\",\"Tapia\")\n",
    "  for phrase in word_list:\n",
    "    tokenized_word.append(tokenizer.tokenize(phrase))\n",
    "  \n",
    "  tokenized_sentence=[]\n",
    "  encoded_token=[]\n",
    "  \n",
    "  for i in range(len(sentence_list)): #tokenize the sentence and encode individual word\n",
    "    token_list=tokenizer.tokenize(sentence_list[i])\n",
    "    tokenized_sentence.append(token_list)\n",
    "    temp_list=['O' for length in range(len(token_list))]\n",
    "    for j in range(len(tokenized_word)):\n",
    "      if all(elem in token_list for elem in tokenized_word[j])==True:\n",
    "        #print(token_list, tokenized_word[j])\n",
    "        for word in tokenized_word[j]:\n",
    "          temp_list[token_list.index(word)]=(type_list[j])\n",
    "          #print(temp_list)\n",
    "    encoded_token.append(temp_list)\n",
    "          \n",
    "  return tokenized_sentence,encoded_token\n",
    "\n",
    "#token_annotator(train_filelist[0])\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_UJDLnnoPQe"
   },
   "outputs": [],
   "source": [
    "def type_token_generator(file): \n",
    "  #this function convert all the text of a record into individual BERT tokenized list and generate type encoding list\n",
    "  all_sentences=note_generator(file)\n",
    "  tokenized_sentences=[]\n",
    "  for sentence in all_sentences:\n",
    "    tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "  \n",
    "  type_token=[]\n",
    "    \n",
    "  sentence_list, encoded_token=token_annotator(file)\n",
    "  \n",
    "  for sentence in tokenized_sentences:\n",
    "    if sentence in sentence_list:\n",
    "      type_token.append(encoded_token[sentence_list.index(sentence)])\n",
    "    else:\n",
    "      type_token.append(['O'for i in range(len(sentence))])\n",
    "  \n",
    "  label_list=[]\n",
    "  #label_dict={\"O\":0, \"DATE\":1, \"DOCTOR\":2,\"HOSPITAL\":3,'PATIENT':4,'AGE':5,'MEDICALRECORD':6,'CITY':7,'STATE':8,'PHONE':9,'USERNAME':10,'IDNUM':11,'PROFESSION':12,'STREET':13,'ZIP':14,'ORGANIZATION':15,'COUNTRY':16,'FAX':17,'DEVICE':18,'EMAIL':19,'LOCATION-OTHER':20,'URL':21,'HEALTHPLAN':22,'BIOID':23}# ,'IPADDRESS':24,'ACCOUNT NUMBER':25}\n",
    "  for type_list in type_token:# we convert the label to numerical for Bert training. We can add types here later. \n",
    "    #label_list.append([label_dict.get(item,item)  for item in type_list])\n",
    "    label_list.append([0 if typetoken =='O' else 1 for typetoken in type_list])\n",
    "\n",
    "\n",
    "  #return tokenized_sentences, type_token, label_list\n",
    "  return tokenized_sentences, type_token, label_list #take a look at segment of the list to make sure the they are corect\n",
    "# we were missing tokenized_sentences, type_token from the return, not sure why\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eztMvTDLey21"
   },
   "source": [
    "### Generating BERT array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXrQFz9jNbCC"
   },
   "outputs": [],
   "source": [
    "def bert_array(file, max_seq_length):\n",
    "  '''This function generates the 5 lists of array that is required to feed into the model'''\n",
    "  \n",
    "  token_sentence, type_token, label_list= type_token_generator(file)\n",
    "  \n",
    "  token_list=[]\n",
    "  input_IDs=[]\n",
    "  input_mask=[]#1 for non padding and 0 for padding\n",
    "  segment_ID=[]\n",
    "  label=[]\n",
    "  \n",
    "  for untrimmed_sentence in token_sentence:\n",
    "    sentence=untrimmed_sentence[0:(max_seq_length)-2] #trim the list to allow space for CLS and SEP\n",
    "    sentence.insert(0,'[CLS]')\n",
    "    sentence.insert(len(sentence),'[SEP]')\n",
    "    length_before_padding=len(sentence)\n",
    "    temp_inputID=[1 for i in range(length_before_padding)]#insert 1 for [CLS] and [SEP] for mask\n",
    "    sentence.extend(['[PAD]' for i in range(max_seq_length-len(sentence))])\n",
    "    temp_inputID.extend([0 for i in range(max_seq_length-len(temp_inputID))])\n",
    "    token_list.append(sentence)\n",
    "    input_mask.append(temp_inputID)\n",
    "    segment_ID.append([0 for i in range(max_seq_length)])\n",
    "  \n",
    "  for token in token_list:\n",
    "    input_ids=tokenizer.convert_tokens_to_ids(token)\n",
    "    input_IDs.append(input_ids)\n",
    "  \n",
    "  for untrimmed_item in label_list:# we assign 0 to be non-PHI (including [CLS] and [SEP], 1 to be PHI and 2 to be padding\n",
    "    item=untrimmed_item[0:(max_seq_length-2)]#trim the list to allow space for CLS and SEP\n",
    "    item.insert(0,0)#class label 24 for CLS (Arnobio - you need to change 24 to 0 for binary)\n",
    "    item.insert(len(item),0) #class label 25 for SEP (Arnobio - you need to change 25 to 0 for binary)\n",
    "    item.extend([0 for i in range(max_seq_length-len(item))])  #class label 26 represents paddinging (ARnobio you need to change 26 to 0 for binary)\n",
    "    label.append(item)\n",
    "  \n",
    "  \n",
    "  return token_list, input_IDs, input_mask, segment_ID, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# token='this'\n",
    "# input_ids = tokenizer.convert_tokens_to_ids(['[CLS]'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "909epvhvxYMN"
   },
   "source": [
    "# Generating data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjhndmGlkYY_"
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 32\n",
    "\n",
    "# create data\n",
    "max_seq_length = 20\n",
    "\n",
    "# num of files to retrieve\n",
    "num_of_file_train = 200\n",
    "\n",
    "\n",
    "def generate_train_data(num_of_file, max_seq_length):#Max number of file number is 789\n",
    "  '''This function runs through a loop to append the tokens, input ids, input masks, segement id and labels to 5 individual np arrays '''\n",
    "  temp_list0, temp_list1, temp_list2, temp_list3, temp_list4=[],[],[],[],[]\n",
    "  for i in range(num_of_file):\n",
    "    temp_data= bert_array(train_filelist[i],max_seq_length)\n",
    "    \n",
    "    for j in range(len(temp_data[0])):\n",
    "      temp_list0.append(temp_data[0][j])\n",
    "      temp_list1.append(temp_data[1][j])\n",
    "      temp_list2.append(temp_data[2][j])\n",
    "      temp_list3.append(temp_data[3][j])\n",
    "      temp_list4.append(temp_data[4][j])\n",
    "  \n",
    "#   np_token_list=np.array(temp_list0)\n",
    "#   np_input_ids=np.array(temp_list1)\n",
    "#   np_input_masks=np.array(temp_list2)\n",
    "#   np_segment_ids=np.array(temp_list3)\n",
    "#   np_labels=np.array(temp_list4)\n",
    "      \n",
    "  #return np_token_list, np_input_ids, np_input_masks, np_segment_ids, np_labels\n",
    "  return temp_list0, temp_list1, temp_list2, temp_list3, temp_list4\n",
    "\n",
    "#change number of file here (MAX:789)\n",
    "\n",
    "\n",
    "train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_labels= generate_train_data(num_of_file_train,max_seq_length)\n",
    "\n",
    "# #print(train_token_list)\n",
    "# #check that the shape is correct\n",
    "# #print(train_input_ids.shape, train_input_masks.shape, train_token_list.shape, train_segment_ids.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YcJYPa7guYAv",
    "outputId": "ec82bacc-7eb4-4d26-9f29-c796ae1d93b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17470\n"
     ]
    }
   ],
   "source": [
    "print(len(train_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_6jcasSriGD"
   },
   "outputs": [],
   "source": [
    "num_of_file_test = 150\n",
    "\n",
    "def generate_test_data(num_of_file, max_seq_length):#Max number of file is 513\n",
    "  '''This function runs through a loop to append the tokens, input ids, input masks, segement id and labels to 5 individual np arrays '''\n",
    "  temp_list0, temp_list1, temp_list2, temp_list3, temp_list4=[],[],[],[],[]\n",
    "  for i in range(num_of_file):\n",
    "    temp_data= bert_array(test_filelist[i],max_seq_length)\n",
    "    \n",
    "    for j in range(len(temp_data[0])):\n",
    "      temp_list0.append(temp_data[0][j])\n",
    "      temp_list1.append(temp_data[1][j])\n",
    "      temp_list2.append(temp_data[2][j])\n",
    "      temp_list3.append(temp_data[3][j])\n",
    "      temp_list4.append(temp_data[4][j])\n",
    "  \n",
    "\n",
    "  return temp_list0, temp_list1, temp_list2, temp_list3, temp_list4\n",
    "\n",
    "test_token_list, test_input_ids, test_input_masks, test_segment_ids, test_labels = generate_test_data(num_of_file_test,max_seq_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hMUhd6L2XXgk",
    "outputId": "7be52e66-fcc5-468d-da6c-ec03373db192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12934\n"
     ]
    }
   ],
   "source": [
    "# print(train_token_list[0])\n",
    " print(len(test_input_ids))\n",
    "# print(train_segment_ids[0])\n",
    "#print(type(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJopx8eaXwdA"
   },
   "source": [
    "### Save Data Arrays for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbqFX-tNSdxh"
   },
   "outputs": [],
   "source": [
    "# save tokens outside o\n",
    "#cd ..\n",
    "np.save(\"train_token_list\",train_token_list)\n",
    "np.save(\"train_input_ids\",train_token_list)\n",
    "np.save(\"train_input_masks\",train_token_list)\n",
    "np.save(\"train_segment_ids\",train_token_list)\n",
    "np.save(\"train_labels\",train_token_list)\n",
    "\n",
    "#%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S68GlWkLdQNQ",
    "outputId": "bc1f2a10-f75f-405f-fd58-fd411ab1b77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "cd gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-q4Pn_DYLFW"
   },
   "source": [
    "### Load Data Arrays from Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSyNIcbGdO1r"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive # this sets the file path to your personal google drive. You will need to enter the authorization code each time. \n",
    "#drive.mount('/gdrive')\n",
    "# cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPGcqi9HwriM"
   },
   "source": [
    "#Buildling the Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h__HPbLpEcPf"
   },
   "outputs": [],
   "source": [
    "# Partially based on and created with teh help with Joachim Rahmfeld and his work, as well as \"BERT in Keras with Tensorflow hub\" (https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b) \n",
    "\n",
    "#BERT_MODEL_HUB\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=1,\n",
    "        pooling=\"sequence_output\",\n",
    "        bert_path=BERT_MODEL_HUB,\n",
    "        #bert_path=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        #             trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "        ]\n",
    "        \n",
    "        trainable_layers = []\n",
    "        #         else:\n",
    "        #             raise NameError(\n",
    "        #                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "        #             )\n",
    "\n",
    "                # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        #         if self.pooling == \"first\":\n",
    "        #             pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"pooled_output\"\n",
    "        #             ]\n",
    "        #         elif self.pooling == \"sequence\":\n",
    "        #             result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #                 \"sequence_output\"\n",
    "        #             ]\n",
    "\n",
    "        #             mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        #             masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "        #                     tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "        #             input_mask = tf.cast(input_mask, tf.float32)\n",
    "        #             pooled = masked_reduce_mean(result, input_mask)\n",
    "        #         else:\n",
    "        #             raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "                \n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "        \n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xo1aPcnGYr2y"
   },
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "\n",
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 24)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, num_labels]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qec3shDEw30"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    #print(in_id, in_mask, in_segment)\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=0, pooling=\"sequence_output\")(bert_inputs)\n",
    "    \n",
    "    #print(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)#random drop out to prevent overfitting\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(2, activation='sigmoid')(dense)#Arnobio: 2 for binary class (not sure why) need to change output shpae to reflect number of classes\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    \n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #losses = custom_loss#added this copying the function from Joachim's notebook\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(loss=losses, optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[custom_acc_orig_tokens])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#build_model(32)\n",
    "# def initialize_vars(sess):\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.tables_initializer())\n",
    "#     K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szPBKLBLw7nD"
   },
   "source": [
    "##Train the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwVb2r25UINh"
   },
   "source": [
    "### Data Preparation\n",
    "Set up data to mimic Joachim's data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hJubB1GFwqeD",
    "outputId": "a56c2cf4-06aa-45b9-bfbe-ec8dd7f8cf16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 17470, 20)\n",
      "(3, 12934, 20)\n"
     ]
    }
   ],
   "source": [
    "#train_token_list, train_input_ids, train_input_masks, train_segment_ids, train_la\n",
    "\n",
    "X_train = np.array([train_input_ids,train_input_masks,train_segment_ids])\n",
    "X_test = np.array([test_input_ids,test_input_masks,test_segment_ids])\n",
    "train_label1=np.array(train_labels)\n",
    "test_label1=np.array(test_labels)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "ZVW_81AGxNez",
    "outputId": "9d74581a-81f9-4668-f211-b3ab26c95c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(9600, 20)\n",
      "(3200, 20)\n",
      "(3200, 20)\n"
     ]
    }
   ],
   "source": [
    "k_start=0\n",
    "k_end_train=9600\n",
    "k_end_dev_start_test=3200\n",
    "k_end_test=k_end_dev_start_test+3200\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "\n",
    "bert_inputs_dev_k = [X_test[0][k_start:k_end_dev_start_test], X_test[1][k_start:k_end_dev_start_test], \n",
    "                      X_test[2][k_start:k_end_dev_start_test]]\n",
    "\n",
    "bert_inputs_test_k = [X_test[0][k_end_dev_start_test:k_end_test], X_test[1][k_end_dev_start_test:k_end_test], \n",
    "                      X_test[2][k_end_dev_start_test:k_end_test]]\n",
    "\n",
    "bert_train_label=train_label1[k_start:k_end_train]\n",
    "bert_dev_label=test_label1[k_start:k_end_dev_start_test]\n",
    "bert_test_label=test_label1[k_end_dev_start_test:k_end_test]\n",
    "\n",
    "print(len(bert_inputs_train_k))\n",
    "print(bert_train_label.shape)\n",
    "print(bert_dev_label.shape)\n",
    "print(bert_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DDeCa11AD2Oh",
    "outputId": "8b8be0e0-1344-4611-e122-c2f6d00a7ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of label 0 is 61723\n",
      "the percent of zero is 0.964421875\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(bert_test_label)):\n",
    "  for j in range (20):\n",
    "    if bert_test_label[i][j]==0:\n",
    "      count+=1\n",
    "\n",
    "print(\"the number of label 0 is\", count)\n",
    "print(\"the percent of zero is\", count/64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0surRGiE614W"
   },
   "outputs": [],
   "source": [
    "#print(bert_inputs_train_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "colab_type": "code",
    "id": "EZmk2uT2EydW",
    "outputId": "175a8e44-968b-46a9-bb90-1065375e3201"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 06:53:20.039936 140462451693440 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0802 06:53:21.104645 140462451693440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2)      514         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,128,774\n",
      "Trainable params: 197,378\n",
      "Non-trainable params: 108,931,396\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9600 samples, validate on 3200 samples\n",
      "Epoch 1/5\n",
      "9600/9600 [==============================] - 1597s 166ms/sample - loss: 0.0670 - custom_acc_orig_tokens: 0.9706 - val_loss: 0.0516 - val_custom_acc_orig_tokens: 0.9815\n",
      "Epoch 2/5\n",
      "9600/9600 [==============================] - 1571s 164ms/sample - loss: 0.0306 - custom_acc_orig_tokens: 0.9892 - val_loss: 0.0337 - val_custom_acc_orig_tokens: 0.9885\n",
      "Epoch 3/5\n",
      "9600/9600 [==============================] - 1584s 165ms/sample - loss: 0.0215 - custom_acc_orig_tokens: 0.9928 - val_loss: 0.0323 - val_custom_acc_orig_tokens: 0.9893\n",
      "Epoch 4/5\n",
      "9600/9600 [==============================] - 1588s 165ms/sample - loss: 0.0167 - custom_acc_orig_tokens: 0.9944 - val_loss: 0.0283 - val_custom_acc_orig_tokens: 0.9910\n",
      "Epoch 5/5\n",
      "9600/9600 [==============================] - 1516s 158ms/sample - loss: 0.0129 - custom_acc_orig_tokens: 0.9958 - val_loss: 0.0294 - val_custom_acc_orig_tokens: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbf948cc630>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "# # #https://stackoverflow.com/questions/34001922/failedpreconditionerror-attempting-to-use-uninitialized-in-tensorflow\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "model = build_model(max_seq_length)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "# num_train_examples = 64\n",
    "# num_dev_examples = 32\n",
    "\n",
    "# model.fit(\n",
    "#     bert_inputs_train_k, \n",
    "#     {\"ner\": labels_train_k },\n",
    "#     validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "#     epochs=8,\n",
    "#     batch_size=32#,\n",
    "#     #callbacks=[tensorboard]\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    bert_train_label,\n",
    "    validation_data=(bert_inputs_dev_k,bert_dev_label),\n",
    "    epochs=5,\n",
    "    batch_size=32#if we change this to input dimension then we solve the 32 problem. \n",
    ")\n",
    "\n",
    "\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIL1kUPOQE3-"
   },
   "source": [
    "#Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGeGerhz7C7e"
   },
   "source": [
    "##Precision/Recall/F1\n",
    "\n",
    "https://stackoverflow.com/questions/1783653/computing-precision-and-recall-in-named-entity-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "VuECrTHSDMCr",
    "outputId": "810da447-dff6-44ee-8e98-41fe3f0c0a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.local_variables_initializer())\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_test_k, \n",
    "    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "P7QW7AdkJYbx",
    "outputId": "374718f2-f0e6-4aea-fdd3-dd51d21a5c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000 64000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.19      0.31     61723\n",
      "           1       0.03      0.61      0.05      2277\n",
      "\n",
      "    accuracy                           0.20     64000\n",
      "   macro avg       0.48      0.40      0.18     64000\n",
      "weighted avg       0.90      0.20      0.30     64000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred_np=np.argmax(result, axis=2)\n",
    "\n",
    "test_np=bert_test_label\n",
    "\n",
    "\n",
    "pred_label=[]\n",
    "test_label=[]\n",
    "\n",
    "for i in range(pred_np.shape[0]):\n",
    "  pred_label.extend(pred_np[i])\n",
    "\n",
    "for i in range(len(test_np)):\n",
    "  test_label.extend(test_np[i])\n",
    "  \n",
    "  \n",
    "print(len(pred_label), len(test_label))\n",
    "\n",
    "print(classification_report(test_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EBniuq-jy1zf",
    "outputId": "611b1ff1-5256-42e7-b101-ff82746d6602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "#save data to gdrive\n",
    "from google.colab import files\n",
    "%cd ..\n",
    "# np.save('result_baseline', result)\n",
    "# files.download('result_baseline.npy')\n",
    "\n",
    "np.save('test_label_baseline',bert_test_label)\n",
    "files.download('test_label_baseline.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5aug2etj0YfA",
    "outputId": "18d4240b-5f3c-4f98-859a-14ffdf9aa9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "np.save('test_label_baseline',bert_test_label)\n",
    "files.download('test_label_baseline.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wBST5SoDl5UX",
    "outputId": "6ef3a77c-ba86-4542-d1ae-8417dc69dd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11605 50118]\n",
      "[ 888 1389]\n"
     ]
    }
   ],
   "source": [
    "con=confusion_matrix(test_label, pred_label)\n",
    "for i in range(len(con)):#print it one by one so it's easier to see\n",
    "  print(con[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "CNtc-pGxi3wZ",
    "outputId": "54833746-fb35-4720-db62-d69de976b528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of 0 in test label is: 994\n",
      "total number of token to be labelled is 1024\n",
      "accuracy if alwasy guess 0 is: 0.970703125\n",
      "total number of when preict and test is the same is: 945\n",
      "accuracy is 0.9228515625\n"
     ]
    }
   ],
   "source": [
    "#Convert all probabiliyt less than 0.5 to 1 else 0\n",
    "for i in range(32):\n",
    "  for j in range(32):\n",
    "    if result[i][j]<=0.5:\n",
    "      result[i][j]=1\n",
    "    else:\n",
    "      result[i][j]=0\n",
    "\n",
    "zero_count=0\n",
    "for i in range(32):\n",
    "  for j in range(32):\n",
    "    if test_label[i][j]==0:\n",
    "      zero_count+=1\n",
    "      \n",
    "count=0\n",
    "for i in range(32):\n",
    "  for j in range(32):\n",
    "    if result[i][j]==test_label[i][j]:\n",
    "      count+=1\n",
    "    \n",
    "    \n",
    "     \n",
    "#print (result[0][1]==test_label[0][1])\n",
    "      \n",
    "print(\"total number of 0 in test label is:\", zero_count)\n",
    "print(\"total number of token to be labelled is\", total)\n",
    "print(\"accuracy if alwasy guess 0 is:\", zero_count/total)\n",
    "\n",
    "print(\"total number of when preict and test is the same is:\", count)\n",
    "total=32*32\n",
    "\n",
    "\n",
    "\n",
    "print(\"accuracy is\", count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRXWzhb95K5o"
   },
   "outputs": [],
   "source": [
    "pred=[[0,0,0,1,2,3,4,5,5,0],[1,4,2,3,0,0,0,0,0,0]]\n",
    "golden=[[1,1,1,5,5,6,2,0,0,0],[0,0,0,0,2,3,4,2,4,0]]\n",
    "\n",
    "def precision(predict, golden):\n",
    "  '''We use the exact match approach as explained in this post: https://stackoverflow.com/questions/1783653/computing-precision-and-recall-in-named-entity-recognition'''\n",
    "  TP=[]\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": " De-Identification Using Bert, Shared (Binary).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
